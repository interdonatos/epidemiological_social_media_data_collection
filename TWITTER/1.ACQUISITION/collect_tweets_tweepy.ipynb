{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "collect_tweets_tweepy",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK9zwYqk8zBD",
        "outputId": "cc2cde87-db5b-417e-d81d-bccbfda718a8"
      },
      "source": [
        "!pip install preprocessor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: preprocessor in /usr/local/lib/python3.7/dist-packages (1.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf5kgZsY8Vx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230ac7eb-fbb4-48a9-e747-36e1d43c74cb"
      },
      "source": [
        "import tweepy\n",
        "from tweepy import *\n",
        " \n",
        "import pandas as pd\n",
        "import csv\n",
        "import re \n",
        "import string\n",
        "import preprocessor as p\n",
        "import json\n",
        " \n",
        "access_token=\"1349002227113930757-gaeVb8dlP0zEttK8NayAakLEN22gOd\"\n",
        "access_token_secret=\"b1Y8CYioe7fzD58zpOXxLXbRawa4zJqXOqxTduCge5qJx\"\n",
        "consumer_key=\"RVk7inZDs8fbvEMi8B3r8Cl85\"\n",
        "consumer_secret=\"8Fy66bblF51smOnQjxrC7ySgbnuqBWN05KDRSAs3adWxeFziIq\"\n",
        " \n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        " \n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        " \n",
        "csvFile = open('file-name_jsonformat_2411.txt', 'a')\n",
        "csvWriter = csv.writer(csvFile)\n",
        "#csvWriter.write('{\"Tweets\": [') #le rajouter manuellement après\n",
        "\n",
        "search_words = \"avian influenza\"      # enter your words\n",
        "new_search = search_words + \" -filter:retweets\"\n",
        "#date_since=\"2021-11-05\"\n",
        "#date_until=\"2021-11-09\"\n",
        "\n",
        "for tweet in tweepy.Cursor(api.search,q=new_search,count=1000,\n",
        "                           lang=\"en\",\n",
        "                           exclude_replies=True,\n",
        "                           include_rts=False,\n",
        "                           #since=date_since,\n",
        "                           #until=date_until,\n",
        "                           since_id=0, \n",
        "                           tweet_mode='extended' \n",
        "                           ).items(1000):\n",
        "    #csvWriter.writerow([tweet.created_at, tweet.full_text.encode('utf-8'),tweet.user.screen_name.encode('utf-8'), tweet.user.location.encode('utf-8')])\n",
        "    #csvWriter.writerow(['{\"id\": \"',[tweet.id],'\", \"date\": \"',[tweet.created_at],'\", \"tweet\": \"',[tweet.full_text.encode('utf-8')],'\" '' end_tweet'])\n",
        "    csvWriter.writerow(['{id:'+str(tweet.id)+'date:'+str(tweet.created_at)+'tweet:'+str(tweet.full_text.encode('utf-8'))+'\" end_tweet}'])\n",
        "    #csvWriter.writerow([tweet.id, tweet.created_at, tweet.full_text.encode('utf-8'),' end_tweet'])\n",
        "#csvWriter.writerow(']}') # #le rajouter manuellement après\n",
        "\n",
        "import re\n",
        "import json\n",
        "#with open('/content/file-name_jsonformat_2411.txt') as json_data:\n",
        "filename='/content/file-name_jsonformat_2411.txt'\n",
        "file=open(filename,encoding=\"utf-8\").read()\n",
        "#file=json_data.read()\n",
        "\n",
        "file=re.sub('\"{','{',file)\n",
        "file=re.sub('}\"','}, ',file)\n",
        "file=re.sub('\"','',file)\n",
        "file=re.sub('^','{\"Tweets\": [',file)\n",
        "file=re.sub('{id:','{\"id\": \"',file)\n",
        "file=re.sub('date:','\", \"date\": \"',file)\n",
        "file=re.sub('(tweet:b(\\'\"?\"?|\"\"?)|tweet:b)','\", \"tweet\": \"',file)\n",
        "file=re.sub(\"' end_tweet\",'\", \"end_tweet\": \"end_tweet\"',file)\n",
        "file=re.sub(\"end_tweet},\",'\", \"end_tweet\": \"end_tweet\"},',file)\n",
        "file=re.sub('\"end_tweet\": \"end_tweet\" {', '\"end_tweet\": \"end_tweet\"',file)\n",
        "#file=re.sub(\"\\'\",\"'\",file)\n",
        "file=re.sub('\\'\",','\",',file)\n",
        "file=re.sub('$',']}',file)\n",
        "file=re.sub('\"}, ]}\\n]}','\"\"}]}',file)\n",
        "file=re.sub('\"\"\",','\",',file)\n",
        "file=re.sub('\"\",','\"',file)\n",
        "#file=re.sub(\"\\\\\",\"\",file)\n",
        "file = file.replace(\"\\\\\", \"\")\n",
        "file=re.sub('\"\"','\"',file)\n",
        "file=re.sub(r\"\\\\n\",\" \",file) #suppression dex \\n\n",
        "file=re.sub(r\"\\\\x[0-9a-z]+\",\"\",file) #suppression des caractères non unicode\n",
        "#file= file.encode(\"ascii\", \"ignore\") #suppression des caractères non ascii\n",
        "open(filename,'w').write(file)\n",
        "\n",
        "#out_file = open(\"/content/test1.json\", \"w\", encoding=\"utf-8\")\n",
        "#json.dump(file, out_file, indent = 4, sort_keys = False)\n",
        "#out_file.close()\n",
        "\n",
        "#import json\n",
        "#with open(\"/content/file-name_jsonformat_2411.txt\", \"r\") as fin:\n",
        "#  content = json.load(fin)\n",
        "#with open(\"stringJson.txt\", \"w\") as fout:\n",
        "#  json.dump(content, fout, indent=1)\n",
        "#json.dumps(file)\n",
        "#with open('/content/file-name_jsonformat_2411.txt', 'w', encoding='utf-8') as f:\n",
        "  #f.write(file)\n",
        "print(type(file))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    }
  ]
}